#!/usr/bin/env python3
"""
Sage L3 ìŠ¤í¬ë¦½íŠ¸ ê²€ìˆ˜ ë„êµ¬

ë‹¤ë¥¸ AI/ì‚¬ëŒì´ ê²€í† í•  ìˆ˜ ìˆë„ë¡ ì½”ë“œ í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±

ì‚¬ìš©:
    python3 lint_scripts.py                    # í˜„ì¬ ë””ë ‰í† ë¦¬ ê²€ì‚¬
    python3 lint_scripts.py --path /some/dir   # íŠ¹ì • ê²½ë¡œ ê²€ì‚¬
    python3 lint_scripts.py --json             # JSON ì¶œë ¥
"""

import argparse
import ast
import io
import json
import re
import sys
import tokenize
from collections import Counter
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple


@dataclass
class Issue:
    """ê²€ì¶œëœ ë¬¸ì œ"""

    file: str
    line: int
    severity: str  # ERROR, WARNING, INFO
    code: str  # E001, W001, I001
    message: str


@dataclass
class LintResult:
    """ê²€ìˆ˜ ê²°ê³¼"""

    file: str
    issues: List[Issue] = field(default_factory=list)
    passed: bool = True


class ScriptLinter:
    """Python ìŠ¤í¬ë¦½íŠ¸ ê²€ìˆ˜ê¸°"""

    _DEFAULT_NORMAL_INFO_EXCLUDE = {"I002", "I003"}

    def __init__(
        self,
        info_level: str = "normal",
        info_codes: Optional[Set[str]] = None,
        info_exclude_codes: Optional[Set[str]] = None,
    ):
        self.issues: List[Issue] = []
        self.info_level = info_level
        self.info_codes = info_codes
        self.info_exclude_codes = info_exclude_codes or set()

    _EXCLUDE_DIRS: Set[str] = {
        "__pycache__",
        ".venv",
        ".git",
        ".mypy_cache",
        ".pytest_cache",
    }
    _REGEX_CONTEXT_RE = re.compile(
        r"\b(compile|search|match|fullmatch|findall|finditer|sub|split)\s*\(",
        re.IGNORECASE,
    )

    def _summarize_results(self, results: List[LintResult]) -> Tuple[int, int, int]:
        """ë¦¬í¬íŠ¸ ìš”ì•½ ìˆ˜ì¹˜ ê³„ì‚°"""
        total_errors = 0
        total_warnings = 0
        total_info = 0

        for result in results:
            for issue in result.issues:
                if issue.severity == "ERROR":
                    total_errors += 1
                elif issue.severity == "WARNING":
                    total_warnings += 1
                else:
                    total_info += 1

        return total_errors, total_warnings, total_info

    def _issue_reason(self, errors: int, warnings: int, info: int) -> str:
        """íŒŒì¼ë³„ í†µê³¼ ì‚¬ìœ  ìš”ì•½"""
        if errors > 0:
            return f"errors:{errors}"
        if warnings > 0:
            return f"warnings:{warnings}"
        if info > 0:
            return f"info:{info}"
        return "clean"

    def _issue_reason_codes(self, issues: List[Issue]) -> Dict[str, Dict[str, int]]:
        """íŒŒì¼ë³„ ì½”ë“œ ì‚¬ìœ  ìš”ì•½ (ì½”ë“œë³„ ë°œìƒ íšŸìˆ˜)"""
        reason: Dict[str, Counter] = {
            "errors": Counter(),
            "warnings": Counter(),
            "info": Counter(),
        }
        for issue in issues:
            if issue.severity == "ERROR":
                reason["errors"][issue.code] += 1
            elif issue.severity == "WARNING":
                reason["warnings"][issue.code] += 1
            else:
                reason["info"][issue.code] += 1
        return {
            "errors": dict(reason["errors"]),
            "warnings": dict(reason["warnings"]),
            "info": dict(reason["info"]),
        }

    def _top_codes(self, issues: List[Issue], top_n: int) -> Dict[str, List[Dict[str, int]]]:
        """ì‹¬ê°ë„ë³„ ìƒìœ„ ì½”ë“œ í†µê³„"""
        reason = self._issue_reason_codes(issues)
        top = {}
        for key in ("errors", "warnings", "info"):
            items = sorted(reason[key].items(), key=lambda kv: (-kv[1], kv[0]))
            top[key] = [{"code": code, "count": count} for code, count in items[:top_n]]
        return top

    def lint_file(self, path: Path) -> LintResult:
        """ë‹¨ì¼ íŒŒì¼ ê²€ìˆ˜"""
        self.issues = []
        result = LintResult(file=str(path))

        try:
            content = path.read_text(encoding="utf-8")
        except Exception as e:
            result.issues.append(
                Issue(
                    file=str(path),
                    line=0,
                    severity="ERROR",
                    code="E000",
                    message=f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}",
                )
            )
            result.passed = False
            return result

        # 1. ë¬¸ë²• ê²€ì‚¬
        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            result.issues.append(
                Issue(
                    file=str(path),
                    line=e.lineno or 0,
                    severity="ERROR",
                    code="E001",
                    message=f"êµ¬ë¬¸ ì˜¤ë¥˜: {e.msg}",
                )
            )
            result.passed = False
            return result

        # 2. í•¨ìˆ˜ ë‚´ë¶€ import ê²€ì‚¬
        self._check_inner_imports(tree, str(path))

        # 3. ì •ê·œì‹ íŒ¨í„´ ê²€ì‚¬
        self._check_regex_patterns(content, str(path))

        # 4. ì¤‘ë³µ í•¨ìˆ˜ëª… ê²€ì‚¬
        self._check_duplicate_functions(tree, str(path))

        # 5. ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” import ê²€ì‚¬
        self._check_unused_imports(tree, str(path))

        # 6. íƒ€ì… íŒíŠ¸ ì¼ê´€ì„± ê²€ì‚¬
        self._check_type_hints(tree, str(path))

        # 7. docstring ê²€ì‚¬
        self._check_docstrings(tree, str(path))

        result.issues = self._filter_info_issues(self.issues)
        result.passed = not any(i.severity == "ERROR" for i in self.issues)
        return result

    def _check_inner_imports(self, tree: ast.AST, filepath: str):
        """í•¨ìˆ˜ ë‚´ë¶€ import ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                for child in ast.walk(node):
                    if isinstance(child, (ast.Import, ast.ImportFrom)):
                        self.issues.append(
                            Issue(
                                file=filepath,
                                line=child.lineno,
                                severity="WARNING",
                                code="W001",
                                message=f"í•¨ìˆ˜ ë‚´ë¶€ import: {self._get_import_name(child)}",
                            )
                        )

    def _get_import_name(self, node) -> str:
        """import ë¬¸ì—ì„œ ëª¨ë“ˆëª… ì¶”ì¶œ"""
        if isinstance(node, ast.Import):
            return ", ".join(a.name for a in node.names)
        elif isinstance(node, ast.ImportFrom):
            return node.module or ""
        return ""

    def _check_regex_patterns(self, content: str, filepath: str):
        """ì •ê·œì‹ íŒ¨í„´ ê²€ì‚¬"""
        lines = content.splitlines()

        for token in tokenize.generate_tokens(io.StringIO(content).readline):
            if token.type != tokenize.STRING:
                continue

            line_no = token.start[0]
            line_text = lines[line_no - 1] if 0 < line_no <= len(lines) else ""
            if not self._is_regex_context_line(line_text):
                continue

            try:
                value = ast.literal_eval(token.string)
            except Exception:
                continue

            if "[]" in value:
                self.issues.append(
                    Issue(
                        file=filepath,
                        line=line_no,
                        severity="WARNING",
                        code="W002",
                        message="ë¹ˆ ë¬¸ì í´ë˜ìŠ¤ []",
                    )
                )

            prefix = self._get_string_prefix(token.string)
            if "r" not in prefix or "f" in prefix:
                continue
            if re.search(r"\\\\[sdwSDW]", value):
                self.issues.append(
                    Issue(
                        file=filepath,
                        line=line_no,
                        severity="ERROR",
                        code="E002",
                        message="raw string ë‚´ ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ ì˜ì‹¬: \\\\s â†’ \\s",
                    )
                )

    def _get_string_prefix(self, literal: str) -> str:
        """ë¬¸ìì—´ ë¦¬í„°ëŸ´ ì ‘ë‘ì‚¬ ì¶”ì¶œ (r, b, f, u ì¡°í•©)"""
        for idx, ch in enumerate(literal):
            if ch in ("'", '"'):
                return literal[:idx].lower()
        return ""

    def _is_regex_context_line(self, line: str) -> bool:
        """ì •ê·œì‹ ë§¥ë½ ë¼ì¸ íŒì •"""
        lowered = line.lower()
        if "re." in line:
            return True
        if "pattern" in lowered or "regex" in lowered:
            return True
        if self._REGEX_CONTEXT_RE.search(lowered):
            return True
        return False

    def _build_parent_map(self, tree: ast.AST) -> Dict[ast.AST, ast.AST]:
        """AST ë¶€ëª¨ ë…¸ë“œ ë§µ ìƒì„±"""
        parent_map: Dict[ast.AST, ast.AST] = {}
        for parent in ast.walk(tree):
            for child in ast.iter_child_nodes(parent):
                parent_map[child] = parent
        return parent_map

    def _filter_info_issues(self, issues: List[Issue]) -> List[Issue]:
        """INFO ì½”ë“œ í•„í„° ì ìš©"""
        if not self.info_codes and not self.info_exclude_codes:
            return issues

        filtered: List[Issue] = []
        for issue in issues:
            if issue.severity != "INFO":
                filtered.append(issue)
                continue
            if self.info_codes and issue.code not in self.info_codes:
                continue
            if issue.code in self.info_exclude_codes:
                continue
            filtered.append(issue)
        return filtered

    def _is_nested_def(self, node: ast.AST, parent_map: Dict[ast.AST, ast.AST]) -> bool:
        """ì¤‘ì²©ëœ í•¨ìˆ˜/í´ë˜ìŠ¤ ì—¬ë¶€ íŒì •"""
        parent = parent_map.get(node)
        while parent:
            if isinstance(parent, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                return True
            parent = parent_map.get(parent)
        return False

    def _check_duplicate_functions(self, tree: ast.AST, filepath: str):
        """ì¤‘ë³µ í•¨ìˆ˜ëª… ê²€ì‚¬"""
        functions = {}
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                if node.name in functions:
                    self.issues.append(
                        Issue(
                            file=filepath,
                            line=node.lineno,
                            severity="ERROR",
                            code="E003",
                            message=f"ì¤‘ë³µ í•¨ìˆ˜ëª…: {node.name} (ì²« ì •ì˜: {functions[node.name]}í–‰)",
                        )
                    )
                else:
                    functions[node.name] = node.lineno

    def _check_unused_imports(self, tree: ast.AST, filepath: str):
        """ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” import ê²€ì‚¬ (AST ê¸°ë°˜)"""
        if self.info_level == "off":
            return

        imports = []
        used_names: Set[str] = set()

        class UsedNameVisitor(ast.NodeVisitor):
            """ì‚¬ìš©ëœ ì´ë¦„ ìˆ˜ì§‘"""

            def visit_Name(self, node: ast.Name) -> None:
                """ì‚¬ìš©ëœ ì‹ë³„ì ê¸°ë¡"""
                if isinstance(node.ctx, ast.Load):
                    used_names.add(node.id)
                self.generic_visit(node)

        UsedNameVisitor().visit(tree)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    name = alias.asname or alias.name.split(".")[0]
                    imports.append((name, node.lineno))
            elif isinstance(node, ast.ImportFrom):
                if node.module == "__future__":
                    continue
                for alias in node.names:
                    if alias.name == "*":
                        continue
                    name = alias.asname or alias.name
                    imports.append((name, node.lineno))

        for name, lineno in imports:
            if name not in used_names:
                self.issues.append(
                    Issue(
                        file=filepath,
                        line=lineno,
                        severity="INFO",
                        code="I001",
                        message=f"ë¯¸ì‚¬ìš© import ì˜ì‹¬: {name}",
                    )
                )

    def _check_type_hints(self, tree: ast.AST, filepath: str):
        """íƒ€ì… íŒíŠ¸ ì¼ê´€ì„± ê²€ì‚¬"""
        if self.info_level == "off":
            return

        parent_map = self._build_parent_map(tree)
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                if self.info_level != "strict":
                    if self._is_nested_def(node, parent_map):
                        continue
                    # ê³µê°œ í•¨ìˆ˜(ì–¸ë”ìŠ¤ì½”ì–´ ë¯¸ì‹œì‘)ì— ë°˜í™˜ íƒ€ì… ì—†ìœ¼ë©´ ê²½ê³ 
                    if node.name.startswith("_") or node.name == "main":
                        continue
                if node.returns is None:
                    self.issues.append(
                        Issue(
                            file=filepath,
                            line=node.lineno,
                            severity="INFO",
                            code="I002",
                            message=f"ë°˜í™˜ íƒ€ì… íŒíŠ¸ ì—†ìŒ: {node.name}()",
                        )
                    )

    def _check_docstrings(self, tree: ast.AST, filepath: str):
        """docstring ê²€ì‚¬"""
        if self.info_level == "off":
            return

        parent_map = self._build_parent_map(tree)
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                if self.info_level != "strict":
                    if self._is_nested_def(node, parent_map):
                        continue
                    if node.name.startswith("_"):
                        continue
                if not ast.get_docstring(node):
                    self.issues.append(
                        Issue(
                            file=filepath,
                            line=node.lineno,
                            severity="INFO",
                            code="I003",
                            message=f"docstring ì—†ìŒ: {node.name}",
                        )
                    )


def lint_directory(
    path: Path,
    info_level: str,
    info_codes: Optional[Set[str]],
    info_exclude_codes: Set[str],
) -> List[LintResult]:
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Python íŒŒì¼ ê²€ìˆ˜"""
    linter = ScriptLinter(
        info_level=info_level,
        info_codes=info_codes,
        info_exclude_codes=info_exclude_codes,
    )
    results = []

    for py_file in sorted(path.rglob("*.py")):
        if any(part in linter._EXCLUDE_DIRS for part in py_file.parts):
            continue
        results.append(linter.lint_file(py_file))

    return results


def format_report(
    results: List[LintResult],
    json_output: bool = False,
    json_format: str = "summary",
    summary_top_n: int = 5,
) -> str:
    """ê²€ìˆ˜ ê²°ê³¼ í¬ë§·"""
    summary_errors = 0
    summary_warnings = 0
    summary_info = 0

    if json_output:
        data_files = []
        file_reasons = []
        for r in results:
            file_errors = 0
            file_warnings = 0
            file_info = 0
            for issue in r.issues:
                if issue.severity == "ERROR":
                    file_errors += 1
                elif issue.severity == "WARNING":
                    file_warnings += 1
                else:
                    file_info += 1
            linter = ScriptLinter()
            reason = linter._issue_reason(file_errors, file_warnings, file_info)
            reason_codes = linter._issue_reason_codes(r.issues)
            file_reasons.append(
                {
                    "file": r.file,
                    "passed": r.passed,
                    "reason": reason,
                    "reason_codes": reason_codes,
                    "counts": {
                        "errors": file_errors,
                        "warnings": file_warnings,
                        "info": file_info,
                    },
                }
            )
            data_files.append(
                {
                    "file": r.file,
                    "passed": r.passed,
                    "reason": reason,
                    "reason_codes": reason_codes,
                    "counts": {
                        "errors": file_errors,
                        "warnings": file_warnings,
                        "info": file_info,
                    },
                    "issues": [
                        {
                            "line": i.line,
                            "severity": i.severity,
                            "code": i.code,
                            "message": i.message,
                        }
                        for i in r.issues
                    ],
                }
            )
        for r in results:
            for issue in r.issues:
                if issue.severity == "ERROR":
                    summary_errors += 1
                elif issue.severity == "WARNING":
                    summary_warnings += 1
                else:
                    summary_info += 1

        all_issues = [issue for r in results for issue in r.issues]
        summary = {
            "file_count": len(results),
            "errors": summary_errors,
            "warnings": summary_warnings,
            "info": summary_info,
            "passed": summary_errors == 0,
            "strict_passed": summary_errors == 0 and summary_warnings == 0,
            "file_reasons": file_reasons,
            "top_codes": ScriptLinter()._top_codes(all_issues, summary_top_n),
        }

        if json_format == "legacy":
            return json.dumps(data_files, ensure_ascii=False, indent=2)
        return json.dumps(
            {"summary": summary, "files": data_files},
            ensure_ascii=False,
            indent=2,
        )

    # í…ìŠ¤íŠ¸ í¬ë§·
    lines = []
    lines.append("=" * 60)
    lines.append("Sage L3 ìŠ¤í¬ë¦½íŠ¸ ê²€ìˆ˜ ë¦¬í¬íŠ¸")
    lines.append("=" * 60)

    total_errors, total_warnings, total_info = ScriptLinter()._summarize_results(results)

    for r in results:
        lines.append(f"\nğŸ“„ {r.file}")
        lines.append("-" * 40)

        if not r.issues:
            lines.append("  âœ… ë¬¸ì œ ì—†ìŒ")
        else:
            for issue in r.issues:
                icon = {"ERROR": "âŒ", "WARNING": "âš ï¸", "INFO": "â„¹ï¸"}.get(
                    issue.severity, "?"
                )
                lines.append(f"  {icon} [{issue.code}] í–‰ {issue.line}: {issue.message}")

    lines.append("\n" + "=" * 60)
    lines.append("ìš”ì•½")
    lines.append("=" * 60)
    lines.append(f"  ê²€ì‚¬ íŒŒì¼: {len(results)}ê°œ")
    lines.append(f"  âŒ ERROR: {total_errors}ê°œ")
    lines.append(f"  âš ï¸  WARNING: {total_warnings}ê°œ")
    lines.append(f"  â„¹ï¸  INFO: {total_info}ê°œ")

    if total_errors > 0:
        lines.append("\nğŸš¨ ERRORê°€ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    elif total_warnings > 0:
        lines.append("\nâš ï¸  WARNINGì´ ìˆìŠµë‹ˆë‹¤. ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    else:
        lines.append("\nâœ… ëª¨ë“  ê²€ì‚¬ í†µê³¼")

    return "\n".join(lines)


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    parser = argparse.ArgumentParser(description="Sage L3 ìŠ¤í¬ë¦½íŠ¸ ê²€ìˆ˜ ë„êµ¬")
    parser.add_argument("--path", type=str, default=".", help="ê²€ì‚¬í•  ê²½ë¡œ")
    parser.add_argument("--json", action="store_true", help="JSON ì¶œë ¥")
    parser.add_argument(
        "--json-format",
        choices=["summary", "legacy"],
        default="summary",
        help="JSON ì¶œë ¥ í˜•ì‹ (summary: ìš”ì•½ í¬í•¨, legacy: ê¸°ì¡´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸)",
    )
    parser.add_argument(
        "--summary-top-n",
        type=int,
        default=5,
        help="JSON summaryì˜ ì‹¬ê°ë„ë³„ ìƒìœ„ ì½”ë“œ ê°œìˆ˜",
    )
    parser.add_argument(
        "--info-level",
        choices=["off", "normal", "strict"],
        default="normal",
        help="INFO ê²€ì‚¬ ê¸°ì¤€ (off=ë¹„í™œì„±, normal=ê¸°ë³¸, strict=ì—„ê²©)",
    )
    parser.add_argument(
        "--info-codes",
        type=str,
        default="",
        help="INFO ì½”ë“œ ì„ íƒ (ì˜ˆ: I001,I003). ì§€ì • ì‹œ í•´ë‹¹ ì½”ë“œë§Œ í¬í•¨",
    )
    parser.add_argument(
        "--info-exclude-codes",
        type=str,
        default="",
        help="ì œì™¸í•  INFO ì½”ë“œ (ì˜ˆ: I002). ì—¬ëŸ¬ ê°œëŠ” ì½¤ë§ˆë¡œ êµ¬ë¶„",
    )
    parser.add_argument("--strict", action="store_true", help="WARNINGë„ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬")

    args = parser.parse_args()

    def parse_code_list(value: str) -> Optional[Set[str]]:
        codes = {code.strip().upper() for code in value.split(",") if code.strip()}
        return codes or None

    info_codes = parse_code_list(args.info_codes)
    info_exclude_codes = parse_code_list(args.info_exclude_codes) or set()
    if (
        args.info_level == "normal"
        and info_codes is None
        and not args.info_exclude_codes
    ):
        info_exclude_codes = ScriptLinter._DEFAULT_NORMAL_INFO_EXCLUDE.copy()

    path = Path(args.path)
    if not path.exists():
        print(f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}", file=sys.stderr)
        sys.exit(2)
    if path.is_file():
        linter = ScriptLinter(
            info_level=args.info_level,
            info_codes=info_codes,
            info_exclude_codes=info_exclude_codes,
        )
        results = [linter.lint_file(path)]
    else:
        results = lint_directory(
            path,
            info_level=args.info_level,
            info_codes=info_codes,
            info_exclude_codes=info_exclude_codes,
        )

    print(
        format_report(
            results,
            json_output=args.json,
            json_format=args.json_format,
            summary_top_n=args.summary_top_n,
        )
    )

    # ì¢…ë£Œ ì½”ë“œ
    has_error = any(
        any(i.severity == "ERROR" for i in r.issues) for r in results
    )
    has_warning = any(
        any(i.severity == "WARNING" for i in r.issues) for r in results
    )

    if has_error:
        sys.exit(2)
    elif args.strict and has_warning:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()
